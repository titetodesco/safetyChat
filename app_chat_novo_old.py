# app_chat.py ‚Äî ESO ‚Ä¢ CHAT (Embeddings-only)
# Vers√£o com patches PT/EN, ‚ÄúSomente Sphera‚Äù, Sum√°rio de Resultados e **Filtros avan√ßados (Location / Description cont√©m)**
# - Busca SEM√ÇNTICA usando embeddings:
#   ‚Ä¢ Sphera:   data/analytics/sphera_embeddings.npz + sphera.parquet
#   ‚Ä¢ GoSee:    data/analytics/gosee_embeddings.npz  + gosee.parquet
#   ‚Ä¢ History:  data/analytics/history_embeddings.npz + history_texts.jsonl
# - Dicion√°rios (sele√ß√£o autom√°tica de idioma): WS / Precursores / CP
# - Uploads: chunk + embeddings em tempo real (Sentence-Transformers)
# - ‚ÄúSomente Sphera‚Äù: c√°lculo local (limiar de **similaridade do cosseno** e √∫ltimos N anos)
# - Sum√°rio ao final: (2) Estat√≠sticas, (3) Visualiza√ß√µes (exemplo), (4) Interpreta√ß√£o + Resumo descritivo
# - NOVO: Filtros avan√ßados: Location (multiselect) e "Description cont√©m" (substring, case-insensitive)

import os
import io
import re
import json
import requests
import numpy as np
import pandas as pd
import streamlit as st
from pathlib import Path
from datetime import datetime, timedelta





# ---------- Contexto (system prompt) ----------
CONTEXT_MD_REL_PATH = Path(__file__).parent / "docs" / "contexto_eso_chat.md"
DATASETS_CONTEXT_FILE = "datasets_context.md"  # opcional

from pathlib import Path
import re

PROMPTS_MD_PATH = Path("data/prompts/prompts.md")

@st.cache_data(show_spinner=False)
def load_prompts_md(md_path: Path):
    """
    L√™ data/prompts/prompts.md e retorna:
    {
      "Texto":  [{"title": "1) ...", "body": "..."} , ...],
      "Upload": [{"title": "1) ...", "body": "..."} , ...]
    }
    Regras:
      - Se√ß√µes: '## Texto' e '## Upload'
      - Items: '### <n>) <t√≠tulo>' seguidos do corpo at√© o pr√≥ximo '###' ou '##'
    """
    if not md_path.exists():
        return {"Texto": [], "Upload": []}

    raw = md_path.read_text(encoding="utf-8")

    # Quebra por grandes se√ß√µes
    sections = re.split(r"(?m)^##\s+", raw)
    data = {"Texto": [], "Upload": []}
    for sec in sections:
        sec = sec.strip()
        if not sec:
            continue
        # primeira linha = nome da se√ß√£o (Texto/Upload)
        first_line, _, rest = sec.partition("\n")
        section_name = first_line.strip()
        if section_name not in ("Texto", "Upload"):
            continue

        # Itens "###"
        parts = re.split(r"(?m)^###\s+", rest)
        for p in parts:
            p = p.strip()
            if not p:
                continue
            title_line, _, body = p.partition("\n")
            title_line = title_line.strip()
            # limpa numera√ß√£o, mas mant√©m no t√≠tulo exibido
            title = title_line
            body = body.strip()
            data[section_name].append({"title": title, "body": body})

    # Ordena por prefixo num√©rico se houver (1), 2), etc.)
    def _key(x):
        m = re.match(r"^(\d+)\)", x["title"])
        return int(m.group(1)) if m else 9999
    for k in data:
        data[k].sort(key=_key)
    return data


@st.cache_data(show_spinner=False)
def load_file_text(p: Path) -> str:
    try:
        return p.read_text(encoding="utf-8")
    except Exception as e:
        return f"[AVISO] N√£o consegui ler {p}: {e} (Prosseguindo sem esse contexto.)"

def build_system_prompt() -> str:
    preambulo = (
        "Voc√™ √© o ESO-CHAT (seguran√ßa operacional)."
        "Siga estritamente as regras e conven√ß√µes do contexto abaixo."
        "Responda em PT-BR por padr√£o."
        "Quando usar buscas sem√¢nticas, sempre mostre IDs/Fonte e similaridade."
        "N√£o invente dados fora dos contextos fornecidos."
    )
    ctx_md = load_file_text(CONTEXT_MD_REL_PATH)
    return preambulo + " === CONTEXTO ESO-CHAT (.md) === " + ctx_md

if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = build_system_prompt()

if st.sidebar.button("Recarregar contexto (.md)"):
    st.session_state.system_prompt = build_system_prompt()
    st.sidebar.success("Contexto recarregado.")
    
# ===== Assistente de Prompts =====
st.sidebar.subheader("Assistente de Prompts")
prompts_bank = load_prompts_md(PROMPTS_MD_PATH)

# Escolha do tipo
prompt_type = st.sidebar.selectbox("Tipo de an√°lise", options=["Texto", "Upload"], index=0)

# Op√ß√µes de prompt para o tipo escolhido
titles = [it["title"] for it in prompts_bank.get(prompt_type, [])]
if not titles:
    st.sidebar.info("Nenhum prompt encontrado em {} (se√ß√£o {}).".format(PROMPTS_MD_PATH, prompt_type))
else:
    selected_title = st.sidebar.selectbox("Modelo de prompt", options=titles, index=0, key="prompt_title_{}".format(prompt_type))
    # Recupera corpo
    selected = next((it for it in prompts_bank[prompt_type] if it["title"] == selected_title), None)
    body = selected["body"] if selected else ""

    # Coloca o corpo do prompt no rascunho (session_state)
    if "draft_prompt" not in st.session_state:
        st.session_state.draft_prompt = ""

    if st.sidebar.button("Carregar no rascunho", use_container_width=True):
        st.session_state["draft_prompt"] = body
        st.sidebar.success("Modelo carregado no rascunho (edite antes de enviar).")
        st.rerun()

# ---------- Config b√°sica ----------
st.set_page_config(page_title="SAFETY ‚Ä¢ CHAT", page_icon="üí¨", layout="wide")

DATA_DIR = "data"
AN_DIR = os.path.join(DATA_DIR, "analytics")
ALT_DIR = "/mnt/data"  # fallback em ambientes gerenciados
ST_MODEL_NAME = os.getenv("ST_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2")

# Modelo de chat (Ollama-compatible). Se n√£o tiver chave, tenta mesmo assim.
OLLAMA_HOST  = st.secrets.get("OLLAMA_HOST", os.getenv("OLLAMA_HOST", "https://ollama.com"))
OLLAMA_MODEL = st.secrets.get("OLLAMA_MODEL", os.getenv("OLLAMA_MODEL", "gpt-oss:20b"))
OLLAMA_API_KEY = st.secrets.get("OLLAMA_API_KEY", os.getenv("OLLAMA_API_KEY"))
HEADERS_JSON = {"Authorization": f"Bearer {OLLAMA_API_KEY}", "Content-Type": "application/json"} if OLLAMA_API_KEY else {"Content-Type": "application/json"}

# ---------- Depend√™ncias necess√°rias ----------
def _fatal(msg: str):
    st.error(msg)
    st.stop()

try:
    from sentence_transformers import SentenceTransformer
except Exception as e:
    _fatal(
        "‚ùå sentence-transformers n√£o est√° dispon√≠vel."
        "Instale as depend√™ncias (incluindo torch CPU) conforme o requirements.txt recomendado."
        f"Detalhe: {e}"
    )

try:
    import pypdf
except Exception:
    pypdf = None

try:
    import docx
except Exception:
    docx = None

# ---------- Utilidades ----------
def ollama_chat(messages, model=OLLAMA_MODEL, temperature=0.2, stream=False, timeout=120):
    payload = {"model": model, "messages": messages, "temperature": float(temperature), "stream": bool(stream)}
    r = requests.post(f"{OLLAMA_HOST}/api/chat", headers=HEADERS_JSON, json=payload, timeout=timeout)
    r.raise_for_status()
    return r.json()

def l2norm(mat: np.ndarray) -> np.ndarray:
    mat = mat.astype(np.float32, copy=False)
    n = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9
    return mat / n

def cos_topk(E_db: np.ndarray, q: np.ndarray, k: int) -> list[tuple[int, float]]:
    if E_db is None or E_db.size == 0 or k <= 0:
        return []
    q = q.astype(np.float32, copy=False)
    q = q / (np.linalg.norm(q) + 1e-9)
    sims = E_db @ q
    idx = np.argsort(-sims)[:k]
    return [(int(i), float(sims[i])) for i in idx]

def load_npz_embeddings(path: str) -> np.ndarray | None:
    if not os.path.exists(path):
        return None
    try:
        with np.load(path, allow_pickle=True) as z:
            for key in ("embeddings", "E", "X", "vectors", "vecs"):
                if key in z:
                    E = np.array(z[key]).astype(np.float32, copy=False)
                    return l2norm(E)
            # fallback: maior matriz 2D
            best_k, best_n = None, -1
            for k in z.files:
                arr = z[k]
                if isinstance(arr, np.ndarray) and arr.ndim == 2 and arr.shape[0] > best_n:
                    best_k, best_n = k, arr.shape[0]
            if best_k is None:
                st.warning(f"{os.path.basename(path)} n√£o cont√©m matriz 2D de embeddings.")
                return None
            E = np.array(z[best_k]).astype(np.float32, copy=False)
            return l2norm(E)
    except Exception as e:
        st.warning(f"Falha ao ler {path}: {e}")
        return None

def read_pdf_bytes(b: bytes) -> str:
    if pypdf is None:
        return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(b))
        out = []
        for pg in reader.pages:
            try:
                out.append(pg.extract_text() or "")
            except Exception:
                pass
        return "".join(out)
    except Exception:
        return ""

def read_docx_bytes(b: bytes) -> str:
    if docx is None:
        return ""
    try:
        doc = docx.Document(io.BytesIO(b))
        return "".join(p.text for p in doc.paragraphs)
    except Exception:
        return ""

def read_any(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith(".pdf"):
        return read_pdf_bytes(data)
    if name.endswith(".docx"):
        return read_docx_bytes(data)
    if name.endswith(".xlsx") or name.endswith(".xls"):
        try:
            xls = pd.ExcelFile(io.BytesIO(data))
            frames = []
            for s in xls.sheet_names:
                df = xls.parse(s)
                frames.append(df.astype(str))
            return pd.concat(frames, axis=0, ignore_index=True).to_csv(index=False) if frames else ""
        except Exception:
            return ""
    if name.endswith(".csv"):
        try:
            df = pd.read_csv(io.BytesIO(data))
            return df.astype(str).to_csv(index=False)
        except Exception:
            return ""
    try:
        return data.decode("utf-8", errors="ignore")
    except Exception:
        return ""

def chunk_text(text: str, max_chars=1200, overlap=200):
    if not text:
        return []
    text = text.replace("", "").replace("", "")
    parts, start, L = [], 0, len(text)
    ov = max(0, min(overlap, max_chars - 1))
    while start < L:
        end = min(L, start + max_chars)
        part = text[start:end].strip()
        if part:
            parts.append(part)
        if end >= L:
            break
        start = max(0, end - ov)
    return parts

def _safe_unpacked(item):
    """Aceita (label, sim) ou (label, sim, suporte). Retorna (label:str, sim:float, support:int|None)."""
    try:
        if isinstance(item, (list, tuple)):
            if len(item) >= 3:
                return str(item[0]), float(item[1]), int(item[2])
            if len(item) >= 2:
                return str(item[0]), float(item[1]), None
        return str(item), None, None
    except Exception:
        return str(item), None, None


def render_dict_tables(dict_matches, md2):
    """
    Anexa em md2 as tr√™s tabelas: WS / Precursores / CP, de forma robusta.
    - Se houver 'suporte' (coluna 3), adiciona a coluna automaticamente.
    - Se a lista estiver vazia, escreve 'Nenhum ‚â• limiar.' em vez de quebrar.
    """
    if dict_matches is None:
        dict_matches = {"ws": [], "prec": [], "cp": []}

    # ---------- WS ----------
    md2 += [
        "",
        "**WS (‚â• limiar, calculado no app)**",
    ]
    ws = dict_matches.get("ws") or []
    if ws:
        md2 += [
            "| Rank | Termo | Similaridade |",
            "|---:|---|---:|",
        ]
        has_sup = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in ws)
        if has_sup:
            md2[-2] = "| Rank | Termo | Similaridade | Suporte |"
            md2[-1] = "|---:|---|---:|---:|"

        for r, item in enumerate(ws, 1):
            label, s, sup = _safe_unpacked(item)
            if s is None:
                md2.append(f"| {r} | {label} |  |")
            else:
                if has_sup and sup is not None:
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                else:
                    md2.append(f"| {r} | {label} | {s:.3f} |")
    else:
        md2 += ["Nenhum WS ‚â• limiar."]

    # ---------- Precursores ----------
    md2 += [
        "",
        "**Precursores (‚â• limiar, calculado no app)**",
    ]
    prec = dict_matches.get("prec") or []
    if prec:
        md2 += [
            "| Rank | Termo | Similaridade |",
            "|---:|---|---:|",
        ]
        has_sup = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in prec)
        if has_sup:
            md2[-2] = "| Rank | Termo | Similaridade | Suporte |"
            md2[-1] = "|---:|---|---:|---:|"

        for r, item in enumerate(prec, 1):
            label, s, sup = _safe_unpacked(item)
            if s is None:
                md2.append(f"| {r} | {label} |  |")
            else:
                if has_sup and sup is not None:
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                else:
                    md2.append(f"| {r} | {label} | {s:.3f} |")
    else:
        md2 += ["Nenhum Precursor ‚â• limiar."]

    # ---------- CP ----------
    md2 += [
        "",
        "**CP (‚â• limiar, calculado no app)**",
    ]
    cp = dict_matches.get("cp") or []
    if cp:
        md2 += [
            "| Rank | Fator | Similaridade |",
            "|---:|---|---:|",
        ]
        has_sup = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in cp)
        if has_sup:
            md2[-2] = "| Rank | Fator | Similaridade | Suporte |"
            md2[-1] = "|---:|---|---:|---:|"

        for r, item in enumerate(cp, 1):
            label, s, sup = _safe_unpacked(item)
            if s is None:
                md2.append(f"| {r} | {label} |  |")
            else:
                if has_sup and sup is not None:
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                else:
                    md2.append(f"| {r} | {label} | {s:.3f} |")
    else:
        md2 += ["Nenhum Fator CP ‚â• limiar."]


# --- Heur√≠stica de idioma (PT/EN) ---
def guess_lang(text: str) -> str:
    if not text:
        return "pt"
    t = text.lower()
    pt_hits = sum(kw in t for kw in [
        " guindaste", " cabo ", " limit switch", "lan√ßa", "conv√©s",
        "devido", "foi decidido", "observado", "pendurado", "equipamento",
        "procedimento", "manuten√ß√£o", "investiga√ß√£o", "faina"
    ])
    en_hits = sum(kw in t for kw in [
        " crane", " wire", " limit switch", "boom", "deck",
        "due to", "decided", "observed", "hanging", "equipment",
        "procedure", "maintenance", "investigation", "sling"
    ])
    return "pt" if pt_hits >= en_hits else "en"

# ---------- Estado ----------
if "chat" not in st.session_state:
    st.session_state.chat = []

if "upld_texts" not in st.session_state:
    st.session_state.upld_texts = []
if "upld_meta" not in st.session_state:
    st.session_state.upld_meta = []
if "upld_emb" not in st.session_state:
    st.session_state.upld_emb = None

if "st_encoder" not in st.session_state:
    st.session_state.st_encoder = None

# ---------- Prefer√™ncias de sa√≠da ----------
st.sidebar.subheader("Sa√≠das (Sum√°rio)")
show_summary = st.sidebar.checkbox("Exibir sum√°rio da consulta", True)
summary_via_model = st.sidebar.checkbox("Resumo descritivo com modelo", True)

# ---------- Carregamento dos cat√°logos ----------
SPH_EMB_PATH = os.path.join(AN_DIR, "sphera_embeddings.npz")
GOS_EMB_PATH = os.path.join(AN_DIR, "gosee_embeddings.npz")
HIS_EMB_PATH = os.path.join(AN_DIR, "history_embeddings.npz")

SPH_PQ_PATH = os.path.join(AN_DIR, "sphera.parquet")
GOS_PQ_PATH = os.path.join(AN_DIR, "gosee.parquet")
HIS_JSONL   = os.path.join(AN_DIR, "history_texts.jsonl")

E_sph = load_npz_embeddings(SPH_EMB_PATH)
E_gos = load_npz_embeddings(GOS_EMB_PATH)
E_his = load_npz_embeddings(HIS_EMB_PATH)

df_sph = None
df_gos = None
rows_his = []

if os.path.exists(SPH_PQ_PATH):
    try:
        df_sph = pd.read_parquet(SPH_PQ_PATH)
    except Exception as e:
        st.warning(f"Falha ao ler {SPH_PQ_PATH}: {e}")
if os.path.exists(GOS_PQ_PATH):
    try:
        df_gos = pd.read_parquet(GOS_PQ_PATH)
    except Exception as e:
        st.warning(f"Falha ao ler {GOS_PQ_PATH}: {e}")
if os.path.exists(HIS_JSONL):
    try:
        with open(HIS_JSONL, "r", encoding="utf-8") as f:
            for line in f:
                rows_his.append(json.loads(line))
    except Exception as e:
        st.warning(f"Falha ao ler {HIS_JSONL}: {e}")

# --- Dicion√°rios PT/EN (caminhos) ---
WS_PT_NPZ = os.path.join(AN_DIR, "ws_embeddings_pt.npz")
WS_EN_NPZ = os.path.join(AN_DIR, "ws_embeddings_en.npz")
WS_PT_LBL_PARQ = os.path.join(AN_DIR, "ws_embeddings_pt.parquet")
WS_EN_LBL_PARQ = os.path.join(AN_DIR, "ws_embeddings_en.parquet")

PREC_PT_NPZ = os.path.join(AN_DIR, "prec_embeddings_pt.npz")
PREC_EN_NPZ = os.path.join(AN_DIR, "prec_embeddings_en.npz")
PREC_PT_LBL_PARQ = os.path.join(AN_DIR, "prec_embeddings_pt.parquet")
PREC_EN_LBL_PARQ = os.path.join(AN_DIR, "prec_embeddings_en.parquet")

CP_NPZ = os.path.join(AN_DIR, "cp_embeddings.npz")
CP_LBL_PARQ = os.path.join(AN_DIR, "cp_labels.parquet")

def load_dict_bank(npz_path: str, labels_parquet: str):
    E = load_npz_embeddings(npz_path)
    labels = None
    if os.path.exists(labels_parquet):
        try:
            labels = pd.read_parquet(labels_parquet)
        except Exception:
            labels = None
    if E is None or labels is None or len(labels) != E.shape[0]:
        return None, None
    return E, labels

def select_ws_bank(lang: str):
    if lang == "en" and os.path.exists(WS_EN_NPZ):
        return load_dict_bank(WS_EN_NPZ, WS_EN_LBL_PARQ)
    return load_dict_bank(WS_PT_NPZ, WS_PT_LBL_PARQ)

def select_prec_bank(lang: str):
    if lang == "en" and os.path.exists(PREC_EN_NPZ):
        return load_dict_bank(PREC_EN_NPZ, PREC_EN_LBL_PARQ)
    return load_dict_bank(PREC_PT_NPZ, PREC_PT_LBL_PARQ)

def select_cp_bank():
    return load_dict_bank(CP_NPZ, CP_LBL_PARQ)

# ---------- Fun√ß√µes de embeddings ----------

def ensure_st_encoder():
    if st.session_state.st_encoder is None:
        try:
            st.session_state.st_encoder = SentenceTransformer(ST_MODEL_NAME)
        except Exception as e:
            _fatal("‚ùå N√£o foi poss√≠vel carregar o encoder de embeddings (Sentence-Transformers). "
        f"Modelo: {ST_MODEL_NAME} Detalhe: {e}"
            )

def encode_texts(texts: list[str], batch_size: int = 64) -> np.ndarray:
    ensure_st_encoder()
    M = st.session_state.st_encoder.encode(
        texts, batch_size=batch_size, show_progress_bar=False,
        convert_to_numpy=True, normalize_embeddings=True
    ).astype(np.float32)
    return M


def aggregate_dict_matches_over_hits(
    hits, lang: str,
    thr_ws: float, thr_prec: float, thr_cp: float,
    topn_ws: int, topn_prec: int, topn_cp: int,
    agg_mode: str = "max",
    per_event_thr: float = 0.30,
    min_support: int = 2,
):
    """
    WS/Precursores/CP somente dos dicion√°rios embutidos vs DESCRIPTIONS dos hits Sphera.
    Agrega por 'max' ou 'mean', aplica limiar por evento e suporte m√≠nimo.
    Retorna dict com listas de tuplas (label, sim, suporte).
    """
    try:
        if not hits:
            return {"ws": [], "prec": [], "cp": []}

        descs = []
        for _, _, row in hits:
            d = str(row.get("Description", row.get("DESCRIPTION", ""))).strip()
            if d:
                descs.append(d)
        if not descs:
            return {"ws": [], "prec": [], "cp": []}

        V_desc = encode_texts(descs, batch_size=32)  # (M, D)
        V_desc_T = V_desc.T

        def _score_bank(E_bank, labels_df, thr_global, topn_target):
            if E_bank is None or labels_df is None or len(labels_df) != E_bank.shape[0]:
                return []
            S = (E_bank @ V_desc_T)  # (N_terms x M_events)
            support = (S >= per_event_thr).sum(axis=1)
            sims = S.mean(axis=1) if agg_mode == "mean" else S.max(axis=1)
            mask = (support >= min_support) & (sims >= thr_global)
            idx = np.where(mask)[0]
            if idx.size == 0:
                return []
            order = idx[np.argsort(sims[idx])[::-1]]
            out = []
            for i in order[:topn_target]:
                label = str(labels_df.iloc[i].get("label", labels_df.iloc[i].get("text", f"TERM_{i}")))
                out.append((label, float(sims[i]), int(support[i])))
            return out

        E_ws, L_ws = select_ws_bank(lang)
        E_pr, L_pr = select_prec_bank(lang)
        E_cp, L_cp = select_cp_bank()

        return {
            "ws":  _score_bank(E_ws, L_ws, thr_ws,  topn_ws),
            "prec": _score_bank(E_pr, L_pr, thr_prec, topn_prec),
            "cp":  _score_bank(E_cp, L_cp, thr_cp,  topn_cp),
        }
    except Exception as e:
        try:
            st.warning(f"[Dict/Hits] Falha ao agregar dicion√°rios sobre hits: {e}")
        except Exception:
            pass
        return {"ws": [], "prec": [], "cp": []}


def encode_query(q: str) -> np.ndarray:
    ensure_st_encoder()
    v = st.session_state.st_encoder.encode([q], convert_to_numpy=True, normalize_embeddings=True)[0].astype(np.float32)
    v /= (np.linalg.norm(v) + 1e-9)
    return v

def get_sphera_location_col(df: pd.DataFrame) -> str | None:
    """
    Retorna a coluna correta para 'Location' na Sphera, por ordem de prefer√™ncia:
    1) LOCATION
    2) FPSO
    3) Location
    4) FPSO/Unidade
    5) Unidade
    (S√≥ cai para AREA/Setor se nada acima existir ‚Äî e avisa no UI.)
    """
    if df is None:
        return None
    preferred = ["LOCATION", "FPSO", "Location", "FPSO/Unidade", "Unidade"]
    fallback  = ["AREA", "Area", "Setor"]
    for c in preferred:
        if c in df.columns:
            return c
    for c in fallback:
        if c in df.columns:
            st.warning(
                "‚ö†Ô∏è Usando '{}' como fallback de Location (colunas LOCATION/FPSO/Location ausentes)."
                .format(c)
            )
            return c
    return None


# ---------- Sidebar ----------
st.sidebar.header("Configura√ß√µes")
with st.sidebar.expander("Modelo de Resposta", expanded=False):
    st.write("Host:", OLLAMA_HOST)
    st.write("Modelo:", OLLAMA_MODEL)
    if not OLLAMA_API_KEY:
        st.info("Sem OLLAMA_API_KEY ‚Äî ok para ambientes locais se o host n√£o exigir auth.")

st.sidebar.subheader("Recupera√ß√£o (Embeddings padr√£o)")
k_sph = st.sidebar.slider("Top-K Sphera", 0, 10, 5, 1)
k_gos = st.sidebar.slider("Top-K GoSee",  0, 10, 5, 1)
k_his = st.sidebar.slider("Top-K Docs",   0, 10, 3, 1)
k_upl = st.sidebar.slider("Top-K Upload", 0, 10, 5, 1)

st.sidebar.subheader("Upload")
chunk_size  = st.sidebar.slider("Tamanho do chunk", 500, 2000, 1200, 50)
chunk_ovlp  = st.sidebar.slider("Overlap do chunk", 50, 600, 200, 10)
upload_raw_max = st.sidebar.slider("Tamanho m√°x. de UPLOAD_RAW (chars)", 300, 8000, 2500, 100)

st.sidebar.subheader("Regras de Escopo")
only_sphera = st.sidebar.checkbox("Somente Sphera (ignorar GoSee/Docs/Upload)", True)
apply_time_filter = st.sidebar.checkbox("Sphera: filtrar √∫ltimos N anos", True)
years_back = st.sidebar.slider("N (anos)", 1, 10, 3, 1)

st.sidebar.subheader("Limiares de Similaridade (0‚Äì1)")
thr_sphera = st.sidebar.slider("Limiar Sphera (Description ‚Äî cos sim)", 0.0, 1.0, 0.25, 0.01)
thr_ws     = st.sidebar.slider("Limiar WS", 0.0, 1.0, 0.25, 0.01)
thr_prec   = st.sidebar.slider("Limiar Precursores", 0.0, 1.0, 0.25, 0.01)
thr_cp     = st.sidebar.slider("Limiar CP", 0.0, 1.0, 0.25, 0.01)

use_catalog = st.sidebar.checkbox("Injetar datasets_context.md", True)

# ---------- Filtros Avan√ßados ‚Äî Sphera ----------
st.sidebar.subheader("Filtros avan√ßados ‚Äî Sphera")
_sph_loc_col = None
_sph_loc_options = []
_sph_has_desc = False
desc_candidates = ["Description", "DESCRIPTION"]
_sph_desc_col = next((c for c in desc_candidates if c in (df_sph.columns if df_sph is not None else [])), None)
if df_sph is not None:
    _sph_loc_col = get_sphera_location_col(df_sph)  # << aqui
    if _sph_loc_col:
        _sph_loc_options = sorted([str(x) for x in df_sph[_sph_loc_col].dropna().unique()])[:500]
    _sph_has_desc = "Description" in df_sph.columns or "DESCRIPTION" in df_sph.columns


sph_loc_selected = st.sidebar.multiselect(
    "Location (se dispon√≠vel)", options=_sph_loc_options, default=[]
) if _sph_loc_col else []

sph_desc_contains = st.sidebar.text_input(
    "Description cont√©m (substring)", value=""
) if _sph_has_desc else ""

uploaded_files = st.sidebar.file_uploader(
    "Upload (PDF, DOCX, XLSX, CSV, TXT/MD)",
    type=["pdf", "docx", "xlsx", "xls", "csv", "txt", "md"],
    accept_multiple_files=True
)

c1, c2 = st.sidebar.columns(2)
with c1:
    if st.button("Limpar uploads", use_container_width=True):
        st.session_state.upld_texts = []
        st.session_state.upld_meta = []
        st.session_state.upld_emb = None
        st.session_state.pop("last_upload_digest", None)
with c2:
    if st.button("Limpar chat", use_container_width=True):
        st.session_state.chat = []

# ---------- Indexa√ß√£o de Uploads ----------
if uploaded_files:
    with st.spinner("Lendo e embutindo uploads (embeddings)‚Ä¶"):
        new_texts, new_meta = [], []
        for uf in uploaded_files:
            try:
                raw = read_any(uf)
                parts = chunk_text(raw, max_chars=chunk_size, overlap=chunk_ovlp)
                for i, p in enumerate(parts):
                    new_texts.append(p)
                    new_meta.append({"file": uf.name, "chunk_id": i})
            except Exception as e:
                st.warning(f"Falha ao processar {uf.name}: {e}")
        if new_texts:
            M_new = encode_texts(new_texts, batch_size=64)
            if st.session_state.upld_emb is None:
                st.session_state.upld_emb = M_new
            else:
                st.session_state.upld_emb = np.vstack([st.session_state.upld_emb, M_new])
            st.session_state.upld_texts.extend(new_texts)
            st.session_state.upld_meta.extend(new_meta)
            st.success(f"Upload indexado: {len(new_texts)} chunks.")

# ---------- Fun√ß√µes de busca / filtros ----------

def filter_sphera_by_date(df: pd.DataFrame, years: int) -> pd.DataFrame:
    if df is None or "EVENT_DATE" not in df.columns:
        return df
    try:
        d = df.copy()
        d["EVENT_DATE"] = pd.to_datetime(d["EVENT_DATE"], errors="coerce")
        cutoff = pd.Timestamp(datetime.utcnow() - timedelta(days=365*years))
        return d[d["EVENT_DATE"] >= cutoff]
    except Exception:
        return df


def apply_advanced_filters(base: pd.DataFrame) -> pd.DataFrame:
    d = base
    if _sph_loc_col and sph_loc_selected:
        d = d[d[_sph_loc_col].astype(str).isin(set(sph_loc_selected))]
    if _sph_has_desc and sph_desc_contains:
        pat = re.escape(sph_desc_contains)
        desc_col = _sph_desc_col or ("Description" if "Description" in d.columns else None)
        if desc_col:
            d = d[d[desc_col].astype(str).str.contains(pat, case=False, na=False)]
    return d

def sphera_similar_to_text(query_text: str, min_sim: float, years: int | None = None, topk: int = 50):
    """Retorna [(event_id, sim, row)] com sim >= min_sim (cosine), usando Sphera/Description e filtros avan√ßados."""
    if df_sph is None or E_sph is None or E_sph.size == 0:
        return []
    base = df_sph
    if years is not None:
        base = filter_sphera_by_date(base, years)
    base = apply_advanced_filters(base)

    text_col = "Description" if "Description" in base.columns else base.columns[0]
    id_col = "Event ID" if "Event ID" in base.columns else ("EVENT_NUMBER" if "EVENT_NUMBER" in base.columns else None)

    # alinhar E_sph com o √≠ndice filtrado (apenas se √≠ndice for inteiro)
    try:
        base_idx = base.index.to_numpy()
        if np.issubdtype(base_idx.dtype, np.integer):
            E_view = E_sph[base_idx, :]
        else:
            raise TypeError("√çndice n√£o inteiro; usando E_sph completo.")
    except Exception:
        E_view = E_sph
        base = df_sph
        base = apply_advanced_filters(base)  # reaplicar se caiu no fallback
        if years is not None:
            base = filter_sphera_by_date(base, years)

    qv = encode_query(query_text)
    sims = E_view @ qv
    idx = np.argsort(-sims)

    out = []
    upto = min(topk, len(idx))
    for i in idx[:upto]:
        s = float(sims[i])
        if s < min_sim:
            break
        row = base.iloc[i]
        evid = row.get(id_col, f"row{i}") if id_col else f"row{i}"
        out.append((evid, s, row))
    return out


def match_from_dicts(query_text: str, lang: str, thr_ws: float, thr_prec: float, thr_cp: float, topk: int = 20):
    out = {"ws": [], "prec": [], "cp": []}

    # WS
    E_ws, L_ws = select_ws_bank(lang)
    if E_ws is not None:
        qv = encode_query(query_text)
        sims = E_ws @ qv
        idx = np.argsort(-sims)
        for i in idx[:min(topk, len(idx))]:
            s = float(sims[i])
            if s < thr_ws:
                break
            label = str(L_ws.iloc[i].get("label", L_ws.iloc[i].get("text", f"WS_{i}")))
            out["ws"].append((label, s))

    # Precursores
    E_pr, L_pr = select_prec_bank(lang)
    if E_pr is not None:
        qv = encode_query(query_text)
        sims = E_pr @ qv
        idx = np.argsort(-sims)
        for i in idx[:min(topk, len(idx))]:
            s = float(sims[i])
            if s < thr_prec:
                break
            label = str(L_pr.iloc[i].get("label", L_pr.iloc[i].get("text", f"Prec_{i}")))
            out["prec"].append((label, s))

    # CP
    E_cp, L_cp = select_cp_bank()
    if E_cp is not None:
        qv = encode_query(query_text)
        sims = E_cp @ qv
        idx = np.argsort(-sims)
        for i in idx[:min(topk, len(idx))]:
            s = float(sims[i])
            if s < thr_cp:
                break
            label = str(L_cp.iloc[i].get("label", L_cp.iloc[i].get("text", f"CP_{i}")))
            out["cp"].append((label, s))

    return out


def get_upload_raw(max_chars: int) -> str:
    if not st.session_state.upld_texts:
        return ""
    buf, total = [], 0
    for t in st.session_state.upld_texts[:3]:
        if total >= max_chars:
            break
        t = t[: max_chars - total]
        buf.append(t)
        total += len(t)
    return "".join(buf).strip()

# (NOVO) Parser simples para blocos do RAG misto

def parse_blocks(blocks: list[str]):
    stats = {
        "Sphera": {"count": 0, "sims": []},
        "GoSee": {"count": 0, "sims": []},
        "Docs":   {"count": 0, "sims": []},
        "Upload": {"count": 0, "sims": []},
    }
    for b in blocks or []:
        if b.startswith("[UPLOAD_RAW]"):
            continue
        m = re.search(r"\(sim=([0-9.]+)\)", b)
        sim = float(m.group(1)) if m else None
        if b.startswith("[Sphera/"):
            stats["Sphera"]["count"] += 1
            if sim is not None: stats["Sphera"]["sims"].append(sim)
        elif b.startswith("[GoSee/"):
            stats["GoSee"]["count"] += 1
            if sim is not None: stats["GoSee"]["sims"].append(sim)
        elif b.startswith("[Docs/"):
            stats["Docs"]["count"] += 1
            if sim is not None: stats["Docs"]["sims"].append(sim)
        elif b.startswith("[UPLOAD "):
            stats["Upload"]["count"] += 1
            if sim is not None: stats["Upload"]["sims"].append(sim)
    return stats

# (NOVO) Fun√ß√µes utilit√°rias para sum√°rio

def _agg_sims(v):
    if not v: return {"n": 0, "min": None, "max": None, "avg": None}
    return {"n": len(v), "min": float(np.min(v)), "max": float(np.max(v)), "avg": float(np.mean(v))}


def render_visual_layout_example():
    st.markdown(
        """
**3. Visualiza√ß√µes que o app oferece (exemplo de layout)**
- Heatmap: *Location √ó Risk Area* (contagem de incidentes)
- S√©rie temporal mensal: n√∫mero de eventos por m√™s (√∫ltimos N anos)
- Top termos WS/Precursores/CP: ranking por similaridade
- Tabela export√°vel: eventos Sphera filtrados com ID, data, descri√ß√£o e similaridade
        """
    )


def render_interpretation_via_model(prompt: str, context_hint: str):
    msgs = [
        {"role": "system", "content": st.session_state.system_prompt},
        {"role": "user", "content": (
            "Voc√™ √© um analista de Seguran√ßa Operacional."
            "Escreva uma interpreta√ß√£o breve e objetiva dos resultados, com 3‚Äì6 bullet points,"
            "indicando padr√µes, poss√≠veis causas (WS/Precursores/CP) e sugest√µes pr√°ticas de follow-up."
            f"Contexto: {context_hint}"
            f"Consulta do usu√°rio: {prompt}"
        )}
    ]
    try:
        msgs.append({"role":"user","content":"Importante: N√ÉO gere novas listas de WS, Precursores ou Fatores CP; apenas interprete as tabelas calculadas pelo app (embeddings dos dicion√°rios sobre as DESCRIPTIONS dos eventos Sphera recuperados)."})
        resp = ollama_chat(msgs, model=OLLAMA_MODEL, temperature=0.2, stream=False)
        return resp.get("message", {}).get("content", "").strip()
    except Exception as e:
        return f"[Interpreta√ß√£o autom√°tica indispon√≠vel] {e}"


def render_descriptive_summary_via_model(prompt: str, stats_text: str):
    msgs = [
        {"role": "system", "content": st.session_state.system_prompt},
        {"role": "user", "content": (
            "Produza um resumo descritivo em 4‚Äì6 linhas sobre a busca realizada,"
            "mencionando fontes com resultados, n√≠vel de similaridade observado e limita√ß√µes,"
            "usando tom t√©cnico e claro." + stats_text + f"Pergunta do usu√°rio: {prompt}"
        )}
    ]
    try:
        resp = ollama_chat(msgs, model=OLLAMA_MODEL, temperature=0.2, stream=False)
        return resp.get("message", {}).get("content", "").strip()
    except Exception as e:
        return f"[Resumo descritivo autom√°tico indispon√≠vel] {e}"


def render_stats_section(title: str, per_source_stats: dict, extra_lines: list[str] | None = None):
    st.markdown(f"**2. {title}**")
    lines = []
    for src in ("Sphera", "GoSee", "Docs", "Upload"):
        s = per_source_stats.get(src, {"count": 0, "sims": []})
        agg = _agg_sims(s["sims"]) if "sims" in s else _agg_sims([])
        lines.append(
            f"- **{src}**: {s['count']} itens | sim avg={agg['avg']:.3f} m√°x={agg['max']:.3f} m√≠n={agg['min']:.3f}" if agg['n']>0 else f"- **{src}**: {s['count']} itens"
        )
    if extra_lines:
        lines.extend(extra_lines)
    st.markdown("".join(lines))

# ---------- Busca mista (com filtros aplicados √† Sphera) ----------

def search_all(query: str) -> list[str]:
    """Embute a query e busca nos 4 conjuntos (Sphera/GoSee/Docs/Upload). Retorna blocos formatados."""
    qv = encode_query(query)
    blocks: list[tuple[float, str]] = []

    # Sphera (apenas quando N√ÉO est√° em 'Somente Sphera') com filtros avan√ßados
    if not only_sphera:
        if k_sph > 0 and E_sph is not None and df_sph is not None and len(df_sph) >= E_sph.shape[0]:
            base = df_sph
            if apply_time_filter:
                base = filter_sphera_by_date(base, years_back)
            base = apply_advanced_filters(base)

            text_col = "Description" if "Description" in base.columns else base.columns[0]
            id_col = "Event ID" if "Event ID" in base.columns else ("EVENT_NUMBER" if "EVENT_NUMBER" in base.columns else None)

            # alinhar E com base filtrada
            try:
                base_idx = base.index.to_numpy()
                if np.issubdtype(base_idx.dtype, np.integer):
                    E_view = E_sph[base_idx, :]
                else:
                    raise TypeError
            except Exception:
                E_view = E_sph
                base = df_sph
                if apply_time_filter:
                    base = filter_sphera_by_date(base, years_back)
                base = apply_advanced_filters(base)

            sims = (E_view @ qv).astype(float)
            ord_idx = np.argsort(-sims)
            kept = 0
            for i in ord_idx:
                if kept >= k_sph: break
                s = float(sims[i])
                if s < thr_sphera:  # aplica limiar de SIMILARIDADE do cosseno
                    continue
                row = base.iloc[int(i)]
                evid = row.get(id_col, f"row{i}") if id_col else f"row{i}"
                snippet = str(row.get(text_col, ""))[:800]
                blocks.append((s, f"[Sphera/{evid}] (sim={s:.3f}){snippet}"))
                kept += 1

    # GoSee
    if not only_sphera:
        if k_gos > 0 and E_gos is not None and df_gos is not None and len(df_gos) >= E_gos.shape[0]:
            text_col = "Observation" if "Observation" in df_gos.columns else df_gos.columns[0]
            id_col = "ID" if "ID" in df_gos.columns else None
            hits = cos_topk(E_gos, qv, k=k_gos)
            for i, s in hits:
                row = df_gos.iloc[i]
                gid = row.get(id_col, f"row{i}") if id_col else f"row{i}"
                snippet = str(row.get(text_col, ""))[:800]
                blocks.append((s, f"[GoSee/{gid}] (sim={s:.3f}){snippet}"))

    # Docs (history)
    if not only_sphera:
        if k_his > 0 and E_his is not None and rows_his:
            hits = cos_topk(E_his, qv, k=k_his)
            for i, s in hits:
                r = rows_his[i]
                src = f"Docs/{r.get('source','?')}/{r.get('chunk_id', 0)}"
                snippet = str(r.get("text", ""))[:800]
                blocks.append((s, f"[{src}] (sim={s:.3f}){snippet}"))

    # Upload
    if not only_sphera:
        if k_upl > 0 and st.session_state.upld_emb is not None and len(st.session_state.upld_texts) == st.session_state.upld_emb.shape[0]:
            hits = cos_topk(st.session_state.upld_emb, qv, k=k_upl)
            for i, s in hits:
                meta = st.session_state.upld_meta[i]
                snippet = st.session_state.upld_texts[i][:800]
                blocks.append((s, f"[UPLOAD {meta['file']} / {meta['chunk_id']}] (sim={s:.3f}){snippet}"))

    blocks.sort(key=lambda x: -x[0])
    return [b for _, b in blocks]


def _send_prompt_to_chat():
    text_to_send = (st.session_state.get("draft_prompt") or "").strip()
    if not text_to_send:
        return
    # adiciona ao hist√≥rico como 'user'
    if "chat" not in st.session_state:
        st.session_state.chat = []
    st.session_state.chat.append({"role": "user", "content": text_to_send})
    # sinaliza para o pipeline do chat processar ap√≥s o rerun
    st.session_state["pending_user_prompt"] = text_to_send
    # limpa o rascunho
    st.session_state["draft_prompt"] = ""
    st.rerun()

# ---------- UI ----------
st.title("SAFETY ‚Ä¢ CHAT ‚Äî HIST + UPLD (Embeddings preferencial) + Dicion√°rios PT/EN")
st.caption("RAG local (Sphera / GoSee / Docs / Upload) + WS/Precursores/CP com sele√ß√£o autom√°tica de idioma.")

# Mostrar hist√≥rico
for m in st.session_state.chat:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])


# === Sa√≠da ‚Äì Dicion√°rios (WS/Prec/CP) ===
st.sidebar.markdown("### Sa√≠da ‚Äì Dicion√°rios (WS/Prec/CP)")
topn_ws  = st.sidebar.slider("Top-N WS", 3, 50, 10, 1)
topn_prec = st.sidebar.slider("Top-N Precursores", 3, 50, 10, 1)
topn_cp  = st.sidebar.slider("Top-N CP", 3, 50, 10, 1)
st.sidebar.markdown("**Agrega√ß√£o sobre eventos recuperados (Sphera)**")
agg_mode = st.sidebar.selectbox("Como agregar similaridade por termo", ["max", "mean"], index=0)
per_event_thr = st.sidebar.slider("Limiar por evento (dicion√°rios)", 0.0, 1.0, 0.30, 0.01)
min_support = st.sidebar.slider("Suporte m√≠nimo (n¬∫ de eventos)", 1, 10, 2, 1)
st.sidebar.markdown("### Modo de Sa√≠da")
output_mode = st.sidebar.selectbox("Layout do resultado", ["Auto", "Investiga√ß√£o", "Aprendizado", "Comportamento", "M√©tricas"], index=0)

prompt = st.chat_input("Digite sua pergunta ou cole seu texto")
if prompt and _is_freq_by_type_intent(prompt) and df_sph is not None:
    render_frequency_by_type(df_sph)
    prompt = None
if not prompt and "pending_user_prompt" in st.session_state:
    prompt = st.session_state.pop("pending_user_prompt")
if prompt:
    st.session_state.chat.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Opcional: injeta um recorte 'cru' do upload (m√°x N chars)
    up_raw = get_upload_raw(upload_raw_max)
    lang = guess_lang((prompt or "") + "" + (up_raw or ""))

    if only_sphera:
        # -------- Fluxo "Somente Sphera" --------
        query_text = up_raw if up_raw else prompt
        years = years_back if apply_time_filter else None

        # 1) Eventos Sphera semelhantes (limiar de similaridade do cosseno)
        hits = sphera_similar_to_text(query_text, thr_sphera, years=years, topk=200)
        loc_col = get_sphera_location_col(df_sph)  # << escolha centralizada
        desc_col = _sph_desc_col or ("Description" if "Description" in (df_sph.columns if df_sph is not None else []) else None)
        
        if hits:
            md = [
                "**Eventos do Sphera (calculado no app, limiar de similaridade aplicado)**",
                "| Event Id | Similaridade (cos) | Location | Description |",
                "|---:|---:|---|---|",
            ]
            for evid, s, row in hits:
                loc = str(row.get(loc_col, "N/D")) if loc_col else "N/D"
                desc_val = str(row.get(desc_col, "")) if desc_col else str(row.get("Description",""))
                desc = desc_val.replace("\n", " ")[:4000]
                md.append("| {} | {:.3f} | {} | {} |".format(evid, s, loc, desc))
            tbl = "\n".join(md)
            with st.chat_message("assistant"):
                st.markdown(tbl)
            st.session_state.chat.append({"role": "assistant", "content": tbl})
        else:
            msg = "Nenhum evento do Sphera com **similaridade do cosseno** ‚â• " + str(thr_sphera)
            with st.chat_message("assistant"):
                st.markdown(msg)
            st.session_state.chat.append({"role": "assistant", "content": msg})

        # 2) Dicion√°rios (WS / Precursores / CP)
        dict_matches = aggregate_dict_matches_over_hits(hits, lang, thr_ws, thr_prec, thr_cp, topn_ws, topn_prec, topn_cp, agg_mode, per_event_thr, min_support)
        md2 = []
        # WS
        if dict_matches["ws"]:
            md2 += [
                "",  # linha em branco antes da tabela
                "**WS (‚â• limiar, calculado no app)**",
                "| Rank | Termo | Similaridade |",
                "|---:|---|---:|",
            ]
            
_ws_support = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in dict_matches.get("ws", []))
if _ws_support:
    md2[-2] = "| Rank | Termo | Similaridade | Suporte |"
    md2[-1] = "|---:|---|---:|---:|"
    for r, item in enumerate(dict_matches.get("ws", []), 1):
        try:
            if isinstance(item, (list, tuple)):
                if len(item) >= 3:
                    label, s, sup = item[0], float(item[1]), int(item[2])
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                elif len(item) >= 2:
                    label, s = item[0], float(item[1])
                    md2.append(f"| {r} | {label} | {s:.3f} |")
                else:
                    md2.append(f"| {r} | {str(item)} |  |")
            else:
                md2.append(f"| {r} | {str(item)} |  |")
        except Exception:
            md2.append(f"| {r} | {str(item)} |  |")
else:
            md2 += [
                "",
                "**WS (‚â• limiar, calculado no app)**",
                "Nenhum WS ‚â• limiar.",
            ]
        
        # Precursores
if dict_matches["prec"]:
            md2 += [
                "",
                "**Precursores (‚â• limiar, calculado no app)**",
                "| Rank | Termo | Similaridade |",
                "|---:|---|---:|",
            ]
            
_prec_support = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in dict_matches.get("prec", []))
if _prec_support:
    md2[-2] = "| Rank | Termo | Similaridade | Suporte |"
    md2[-1] = "|---:|---|---:|---:|"
    for r, item in enumerate(dict_matches.get("prec", []), 1):
        try:
            if isinstance(item, (list, tuple)):
                if len(item) >= 3:
                    label, s, sup = item[0], float(item[1]), int(item[2])
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                elif len(item) >= 2:
                    label, s = item[0], float(item[1])
                    md2.append(f"| {r} | {label} | {s:.3f} |")
                else:
                    md2.append(f"| {r} | {str(item)} |  |")
            else:
                md2.append(f"| {r} | {str(item)} |  |")
        except Exception:
            md2.append(f"| {r} | {str(item)} |  |")
else:
            md2 += [
                "",
                "**Precursores (‚â• limiar, calculado no app)**",
                "Nenhum Precursor ‚â• limiar.",
            ]
        
        # CP
if dict_matches["cp"]:
            md2 += [
                "",
                "**CP (‚â• limiar, calculado no app)**",
                "| Rank | Fator | Similaridade |",
                "|---:|---|---:|",
            ]
            
_cp_support = any(isinstance(x, (list, tuple)) and len(x) >= 3 for x in dict_matches.get("cp", []))
if _cp_support:
    md2[-2] = "| Rank | Fator | Similaridade | Suporte |"
    md2[-1] = "|---:|---|---:|---:|"
    for r, item in enumerate(dict_matches.get("cp", []), 1):
        try:
            if isinstance(item, (list, tuple)):
                if len(item) >= 3:
                    label, s, sup = item[0], float(item[1]), int(item[2])
                    md2.append(f"| {r} | {label} | {s:.3f} | {sup} |")
                elif len(item) >= 2:
                    label, s = item[0], float(item[1])
                    md2.append(f"| {r} | {label} | {s:.3f} |")
                else:
                    md2.append(f"| {r} | {str(item)} |  |")
            else:
                md2.append(f"| {r} | {str(item)} |  |")
        except Exception:
            md2.append(f"| {r} | {str(item)} |  |")
else:
            md2 += [
                "",
                "**CP (‚â• limiar, calculado no app)**",
                "Nenhum CP ‚â• limiar.",
            ]
        
if md2:
        out2 = "\n".join(md2)        # ‚Üê AGORA COM QUEBRAS
        with st.chat_message("assistant"):
             st.markdown(out2)
        st.session_state.chat.append({"role": "assistant", "content": out2})

        md2_lines = []
        if dict_matches["ws"]:
            md2_lines.append("")  # linha em branco antes da tabela
            md2_lines.append("**WS (‚â• limiar, calculado no app)**")
            md2_lines.append("| Rank | Termo | Similaridade |")
            md2_lines.append("|---:|---|---:|")
            for r_idx, (label, s) in enumerate(dict_matches["ws"], 1):
                md2_lines.append(f"| {r_idx} | {label} | {s:.3f} |")
        else:
            md2_lines.append("")
            md2_lines.append("**WS (‚â• limiar, calculado no app)**")
            md2_lines.append("Nenhum WS ‚â• limiar.")

        
        # 3) Coment√°rio do LLM sobre os resultados (sem buscar fora)
        msgs = [{"role": "system", "content": st.session_state.system_prompt}]
        if use_catalog and os.path.exists(DATASETS_CONTEXT_FILE):
            try:
                with open(DATASETS_CONTEXT_FILE, "r", encoding="utf-8") as f:
                    msgs.append({"role": "system", "content": f.read()})
            except Exception:
                pass
        msgs.append({"role": "user", "content": f"Explique, sem buscar outras fontes, os resultados calculados no app. Limiar Sphera={thr_sphera}, anos={'todos' if not years else years}."})
        msgs.append({
          "role": "user",
          "content": (
              "Regra obrigat√≥ria (Sphera): Location deve vir da coluna LOCATION, "
              "ou do campo FPSO quando LOCATION n√£o existir; nunca usar AREA como Location. "
              "Se a coluna n√£o existir nos blocos, retornar 'N/D'."
          )
        })
        msgs.append({
          "role": "user",
          "content": (
            "Formate a sa√≠da em tr√™s se√ß√µes separadas com tabelas Markdown, sem texto entre elas, "
            "seguindo exatamente o padr√£o do contexto: "
            "1) **WS (‚â• limiar, calculado no app)**, 2) **Precursores (‚â• limiar, calculado no app)**, "
            "3) **CP (‚â• limiar, calculado no app)**. "
            "Use cabe√ßalho de tabela e 3 casas decimais na similaridade. "
            "Se uma categoria n√£o tiver itens, escreva ‚ÄòNenhum <categoria> ‚â• limiar.‚Äô"
          )
        })

        with st.chat_message("assistant"):
            with st.spinner("Consultando o modelo (an√°lise explicativa)‚Ä¶"):
                try:
                    resp = ollama_chat(msgs, model=OLLAMA_MODEL, temperature=0.2, stream=False)
                    content = resp.get("message", {}).get("content", "").strip() or json.dumps(resp)[:1200]
                except Exception as e:
                    content = f"[Coment√°rio do modelo indispon√≠vel] {e}"
                st.markdown(content)
        st.session_state.chat.append({"role": "assistant", "content": content})

        # 4) SUM√ÅRIO
        if show_summary:
            sims = [s for _, s, _ in hits] if hits else []
            per_source = {
                "Sphera": {"count": len(sims), "sims": sims},
                "GoSee": {"count": 0, "sims": []},
                "Docs": {"count": 0, "sims": []},
                "Upload": {"count": len(st.session_state.upld_texts) if st.session_state.upld_texts else 0, "sims": []},
            }
            extra = [
                f"- Filtro temporal: {'sem filtro' if years is None else f'√∫ltimos {years} anos'}",
                f"- Limiar de similaridade aplicado: {thr_sphera:.2f}",
                f"- Idioma inferido: {lang.upper()}",
                (f"- Location: {', '.join(sph_loc_selected)}" if sph_loc_selected else "- Location: (sem filtro)"),
                (f"- Description cont√©m: '{sph_desc_contains}'" if sph_desc_contains else "- Description cont√©m: (vazio)"),
                f"- WS/Prec/CP retornados: {len(dict_matches['ws'])}/{len(dict_matches['prec'])}/{len(dict_matches['cp'])}",
            ]
            with st.chat_message("assistant"):
                render_stats_section("Estat√≠sticas principais geradas", per_source, extra)
                render_visual_layout_example()
                if summary_via_model:
                    context_hint = f"Sphera hits={len(sims)}, thr={thr_sphera}, years={'all' if years is None else years}"
                    interp = render_interpretation_via_model(prompt, context_hint)
                else:
                    interp = (
                        "- Similaridades indicam proximidade textual com descri√ß√µes Sphera;"
                        "- Ajuste de limiar pode aumentar precis√£o (‚Üë) ou abrang√™ncia (‚Üì);"
                        "- Verificar manualmente top eventos;"
                        "- Revisar WS/Precursores/CP com maior similaridade para a√ß√µes preventivas."
                    )
                st.markdown("**4. Interpreta√ß√£o dos resultados (exemplo t√≠pico)**" + interp)

                stats_text = "".join(extra)
                if summary_via_model:
                    desc = render_descriptive_summary_via_model(prompt, stats_text)
                else:
                    desc = (
                        "Foram retornados eventos do Sphera acima do limiar de similaridade definido, "
                        "considerando o escopo e filtros aplicados. As correspond√™ncias em WS, "
                        "Precursores e CP refor√ßam a leitura contextual e subsidiam decis√µes de risco."
                    )
                st.markdown("**Resumo descritivo da consulta**" + desc)

else:
        # -------- Fluxo RAG ‚Äúcl√°ssico‚Äù --------
        blocks = search_all(prompt)
        up_raw = get_upload_raw(upload_raw_max)
        if up_raw:
            blocks = [f"[UPLOAD_RAW]{up_raw}"] + blocks

        msgs = [{"role": "system", "content": st.session_state.system_prompt}]
        if use_catalog and os.path.exists(DATASETS_CONTEXT_FILE):
            try:
                with open(DATASETS_CONTEXT_FILE, "r", encoding="utf-8") as f:
                    msgs.append({"role": "system", "content": f.read()})
            except Exception:
                pass

        if blocks:
            ctx = "".join(blocks)
            msgs.append({"role": "user", "content": f"CONTEXTOS (HIST + UPLOAD):{ctx}"})
            msgs.append({"role": "user", "content": f"PERGUNTA: {prompt}"})
        else:
            msgs.append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            with st.spinner("Consultando o modelo‚Ä¶"):
                try:
                    resp = ollama_chat(msgs, model=OLLAMA_MODEL, temperature=0.2, stream=False)
                    content = resp.get("message", {}).get("content", "").strip() or json.dumps(resp)[:1200]
                except Exception as e:
                    content = f"Falha ao consultar o modelo: {e}"
                st.markdown(content)
        st.session_state.chat.append({"role": "assistant", "content": content})

        # SUM√ÅRIO
        if show_summary:
            blocks_wo_raw = [b for b in blocks if not b.startswith("[UPLOAD_RAW]")]
            per_source = parse_blocks(blocks_wo_raw)
            extra = [
                f"- Top-K: Sphera={k_sph}, GoSee={k_gos}, Docs={k_his}, Upload={k_upl}",
                f"- Limiar WS/Prec/CP: {thr_ws:.2f}/{thr_prec:.2f}/{thr_cp:.2f}",
                f"- Idioma inferido: {lang.upper()}",
                (f"- Location: {', '.join(sph_loc_selected)}" if sph_loc_selected else "- Location: (sem filtro)"),
                (f"- Description cont√©m: '{sph_desc_contains}'" if sph_desc_contains else "- Description cont√©m: (vazio)"),
                f"- Uploads indexados: {len(st.session_state.upld_texts)} chunks" if st.session_state.upld_texts else "- Sem uploads no contexto",
            ]
            with st.chat_message("assistant"):
                render_stats_section("Estat√≠sticas principais geradas", per_source, extra)
                render_visual_layout_example()
                if summary_via_model:
                    context_hint = (
                        f"Sphera n={per_source['Sphera']['count']} avg={_agg_sims(per_source['Sphera']['sims'])['avg']}; "
                        f"GoSee n={per_source['GoSee']['count']}; Docs n={per_source['Docs']['count']}; "
                        f"Upload n={per_source['Upload']['count']}"
                    )
                    interp = render_interpretation_via_model(prompt, context_hint)
                else:
                    interp = (
                        "- Resultados agregam m√∫ltiplas fontes com base em similaridade;"
                        "- Priorize itens com maior similaridade do cosseno e origem Sphera;"
                        "- Use WS/Prec/CP como apoio a a√ß√µes corretivas/preventivas;"
                        "- Ajuste Top-K/limiares para refinar o escopo."
                    )
                st.markdown("**4. Interpreta√ß√£o dos resultados (exemplo t√≠pico)**" + interp)

                stats_text = "".join(extra)
                if summary_via_model:
                    desc = render_descriptive_summary_via_model(prompt, stats_text)
                else:
                    desc = (
                        "A consulta integrou Sphera, GoSee, Docs e Uploads segundo os Top-K e filtros definidos. "
                        "As similaridades mais altas (cosseno) indicam proximidade textual e relev√¢ncia operacional. "
                        "Ajustes de limiar/Top-K podem ampliar ou reduzir a abrang√™ncia."
                    )
                st.markdown("**Resumo descritivo da consulta**" + desc)

st.markdown("### üìù Rascunho do prompt (edite antes de enviar)")
st.caption("Dica: cole o seu texto do evento onde indicado; se for usar upload, envie os arquivos na barra lateral antes de enviar.")

draft = st.text_area("Conte√∫do do prompt", height=220, key="draft_prompt")

c_a, c_c = st.columns([1,3])
with c_a:
    st.button("Enviar para o chat", use_container_width=True, on_click=_send_prompt_to_chat)
# (sem bot√£o de limpar ‚Äî o _send_prompt_to_chat j√° limpa o rascunho)


# ---------- Painel / Diagn√≥stico ----------
debug = st.sidebar.checkbox("Mostrar painel de diagn√≥stico", False)

if debug:
    with st.expander("üì¶ Status dos √≠ndices", expanded=False):
        def _ok(x): return "‚úÖ" if x else "‚Äî"
        st.write("Sphera embeddings:", _ok(E_sph is not None and df_sph is not None))
        if E_sph is not None and df_sph is not None:
            st.write(f" ‚Ä¢ shape: {E_sph.shape} | linhas df: {len(df_sph)}")
        st.write("GoSee embeddings :", _ok(E_gos is not None and df_gos is not None))
        if E_gos is not None and df_gos is not None:
            st.write(f" ‚Ä¢ shape: {E_gos.shape} | linhas df: {len(df_gos)}")
        st.write("Docs embeddings  :", _ok(E_his is not None and len(rows_his) > 0))
        if E_his is not None and rows_his:
            st.write(f" ‚Ä¢ shape: {E_his.shape} | chunks: {len(rows_his)}")
        st.write("Uploads indexados:", len(st.session_state.upld_texts))
        st.write("Encoder ativo    :", ST_MODEL_NAME)

    with st.expander("üîé Vers√µes dos pacotes", expanded=False):
        import importlib, sys
        pkgs = [
            ("torch", "torch"),
            ("transformers", "transformers"),
            ("sentence-transformers", "sentence_transformers"),
            ("pandas", "pandas"),
            ("numpy", "numpy"),
            ("pyarrow", "pyarrow"),
            ("pypdf", "pypdf"),
            ("python-docx", "docx"),
            ("scikit-learn", "sklearn"),
        ]
        st.write("Python:", sys.version)
        for disp, mod in pkgs:
            try:
                m = importlib.import_module(mod)
                ver = getattr(m, "__version__", "sem __version__")
                st.write(f"{disp}: {ver}")
            except Exception as e:
                st.write(f"{disp}: n√£o instalado ({e})")


def _is_freq_by_type_intent(text: str) -> bool:
    t = (text or "").lower()
    keys = ["frequ√™ncia", "frequencia", "frequency", "freq", "por tipo", "event type", "observation", "near miss", "incident"]
    return any(k in t for k in keys)

def render_frequency_by_type(df_sph):
    type_cols = ["event_type", "EVENT_TYPE", "tipo", "Tipo", "TYPE"]
    col = next((c for c in type_cols if c in df_sph.columns), None)
    if not col:
        st.warning("N√£o encontrei coluna de tipo de evento (ex.: event_type).")
        return

    s = df_sph[col].astype(str).str.strip().str.lower()
    map_alias = {
        "observation": "Observation",
        "near miss": "Near Miss",
        "incident": "Incident",
        "incidente": "Incident",
        "quase acidente": "Near Miss",
        "observa√ß√£o": "Observation",
    }
    s = s.map(lambda x: map_alias.get(x, x.title()))

    freq = s.value_counts().rename_axis("Tipo").reset_index(name="Contagem")
    total = int(freq["Contagem"].sum()) if not freq.empty else 0
    if total == 0:
        st.info("N√£o h√° eventos na base para calcular frequ√™ncia por tipo.")
        return
    freq["Propor√ß√£o"] = (freq["Contagem"] / total).round(3)

    md = []
    md += ["**Frequ√™ncia por tipo (Sphera)**", ""]
    md += ["| Tipo | Contagem | Propor√ß√£o |", "|---|---:|---:|"]
    for _, r in freq.iterrows():
        md.append(f"| {{r['Tipo']}} | {{int(r['Contagem'])}} | {{r['Propor√ß√£o']:.3f}} |")

    out = "\\n".join(md)
    with st.chat_message("assistant"):
        st.markdown(out)
    st.session_state.chat.append({{"role": "assistant", "content": out}})
